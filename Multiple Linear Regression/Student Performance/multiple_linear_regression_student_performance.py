# -*- coding: utf-8 -*-
"""Multiple Linear Regression - Student Performance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EfHhxtYEMIJheXZcOQz3gWbMRVFWXcUq
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

df  = pd.read_csv('/content/Student_Performance.csv')

df

df.columns

# Drop unnecessary columns
# df.drop(['race/ethnicity', 'parental level of education', 'lunch', 'test preparation course'], axis=1, inplace=True)

df.head()

# Check null values
df.isnull().sum()

# Convert boolean values to integers
# Mapping 'Yes' to 1 and 'No' to 0
df['Extracurricular Activities'] = df['Extracurricular Activities'].map({'Yes': 1, 'No': 0})

df.drop('Extracurricular Activities Encoded', axis=1, inplace=True)

import seaborn as sns
sns.pairplot(df)

df.corr()

df.describe()

## V;isuaize the datapoints more closely
plt.scatter(df['Previous Scores'], df['Performance Index'])
plt.xlabel('Previous Scores')
plt.ylabel('Performance Index')
plt.title('Previous Scores vs Performance Index')

## Independent and Dependent features
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

X.head()

y

# Train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

import seaborn as sns
import matplotlib.pyplot as plt

# Example data
# sns.regplot(x='Previous Scores', y='Performance Index', data=df, line_kws={'color': 'red'})

# Plot with a customized color for the regression line
sns.regplot(x='Previous Scores', y='Performance Index', data=df, line_kws={'color': 'orange'})

# Show the plot
plt.show()

# Standarization
from sklearn.preprocessing import StandardScaler
scalar = StandardScaler()

X_train = scalar.fit_transform(X_train)
X_test = scalar.transform(X_test)

X_train

from sklearn.linear_model import LinearRegression
regression = LinearRegression()

regression.fit(X_train, y_train)

# Cross Validation
from sklearn.model_selection import cross_val_score
validation_score = cross_val_score(regression, X_train, y_train, scoring='neg_mean_squared_error',cv=3)

validation_score

np.mean(validation_score)

## Prediction

y_pred = regression.predict(X_test)

y_pred

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('Mean Squared Error:', mse)
print('Mean Absolute Error:', mae)
print('R-squared:', r2)

# Display adjusted R-Squared
adjusted_r2 = 1 - (1 - r2) * (len(y_test) - 1)/ (len(y_test) - X_test.shape[1] - 1)

print('Adjusted R-squared:', adjusted_r2)

## Assumptions
plt.scatter(y_test, y_pred)
plt.xlabel('Actual Performance Index')
plt.ylabel('Predicted Performance Index')
plt.title('Actual vs Predicted Performance Index')

residual = y_test - y_pred
print(residual)

# Plot this residual
sns.distplot(residual)

## Scatter plot with respedct to prediction and residuals
plt.scatter(y_pred, residual)
plt.xlabel('Predicted Performance Index')
plt.ylabel('Residual')
plt.title('Residual Plot')

# Implementing OLS
import statsmodels.api as sm
model = sm.OLS(y_train, X_train).fit()

model.summary()

print(regression.coef_) # Linear Reg coeff

