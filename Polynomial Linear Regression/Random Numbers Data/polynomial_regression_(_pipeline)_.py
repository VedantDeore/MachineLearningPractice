# -*- coding: utf-8 -*-
"""Polynomial Regression ( Pipeline) .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NMxYxvM0OIyNj-ecCUpHysK8J5R28F3b
"""

# Commented out IPython magic to ensure Python compatibility.
# import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline

X = 6 *np.random.rand(100,1) -3 # generating random
 y = 0.5 * X**2 + 1.5* X + 2 + np.random.randn(100,1)
 # quadraic equation used y = 0.5 x^2 + 1.5x + 2 + outliers

X

y

plt.scatter(X,y, color = 'g')
plt.xlabel('X')
plt.ylabel('y')

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)

X_train.shape

## Lets implement Simple Linear Regression
from sklearn.linear_model import LinearRegression
regression_1 = LinearRegression()
regression_1.fit(X_train, y_train)

from sklearn.metrics import r2_score
score = r2_score(y_test, regression_1.predict(X_test))
print(score)
# Less accuracy as the line is a straight line

# Lets visualize this model
plt.plot(X_train, regression_1.predict(X_train), color = 'r')
plt.scatter(X_train, y_train)
plt.xlabel('X')
plt.ylabel('y')

#Lets apply Ploynomial Transformation
# h0(x) = Bo + B1 x1 + B2 x1^2
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree = 2, include_bias = True)
# bias = True,so below is considered
# h0(x) = Bo * 1 + B1 x1 + B2 x1^2
X_train_poly = poly.fit_transform(X_train) # fit makes sure that the main data validation is train data no test refrence is used
X_test_poly = poly.transform(X_test) # apply the technique in test data

X_train_poly
# 1 + Bo X1 + B1 X1^2

from sklearn.metrics import r2_score
regression_2 = LinearRegression()
regression_2.fit(X_train_poly, y_train)
y_pred = regression_2.predict(X_test_poly)
score = r2_score(y_test, y_pred)
print(score)

print(regression_2.coef_)

print(regression_2.intercept_)

plt.scatter(X_train,regression_2.predict(X_train_poly), color = 'g')
plt.scatter(X_train, y_train)
plt.xlabel('X')
plt.ylabel('y')

# For degree =3
poly3 = PolynomialFeatures(degree = 3, include_bias = True)
# bias = True,so below is considered
# h0(x) = Bo * 1 + B1 x1 + B2 x1^2
X_train_poly3 = poly3.fit_transform(X_train) # fit makes sure that the main data validation is train data no test refrence is used
X_test_poly3 = poly3.transform(X_test) # apply the technique in test data

from sklearn.metrics import r2_score
regression_3 = LinearRegression()
regression_3.fit(X_train_poly3, y_train)
y_pred = regression_3.predict(X_test_poly3)
score = r2_score(y_test, y_pred)
print(score)

plt.scatter(X_train,regression_3.predict(X_train_poly3), color = 'g')
plt.scatter(X_train, y_train)
plt.xlabel('X')
plt.ylabel('y')

# For degree = 4
poly4 = PolynomialFeatures(degree = 4, include_bias = True)
# bias = True,so below is considered
# h0(x) = Bo * 1 + B1 x1 + B2 x1^2
X_train_poly4 = poly4.fit_transform(X_train) # fit makes sure that the main data validation is train data no test refrence is used
X_test_poly4 = poly4.transform(X_test) # apply the technique in test data

regression_4 = LinearRegression()
regression_4.fit(X_train_poly4, y_train)
y_pred = regression_4.predict(X_test_poly4)
score = r2_score(y_test, y_pred)
print(score)

plt.scatter(X_train,regression_4.predict(X_train_poly4), color = 'g')
plt.scatter(X_train, y_train)
plt.xlabel('X')
plt.ylabel('y')

# Prediction of new data set
X_new = np.linspace(-3,3,200).reshape(200,1)
X_new_poly = poly.transform(X_new)

X_new_poly

y_new = regression_2.predict(X_new_poly)
plt.plot(X_new, y_new, 'r-', linewidth=2, label = "New Predictions")
plt.plot(X_train,y_train,'b.', label = "Training Points")
plt.plot(X_test, y_test, 'g.', label = "Testing Points")
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()

"""**Pipeline Concepts**

"""

from sklearn.pipeline import Pipeline

def poly_regression(degree):
  X_new = np.linspace(-3,3,200).reshape(200,1)
  X_new_poly = poly.transform(X_new)
  poly_features = PolynomialFeatures(degree = degree, include_bias = True)
  lin_reg = LinearRegression()
  polynomial_regression = Pipeline([
      ("poly_features", poly_features),
      ("lin_reg", lin_reg)
  ])
  polynomial_regression.fit(X_train,y_train) ## Ploynomail and fit of linear regression
  y_pred_new = polynomial_regression.predict(X_new)
  # Plotting prediction line
  plt.plot(X_new, y_pred_new, 'r', label = "Degree" + str(degree), linewidth = 2)
  plt.plot(X_train,y_train,'b.', label = "Training Points")
  plt.plot(X_test, y_test, 'g.', label = "Testing Points")
  plt.xlabel('X')
  plt.ylabel('y')
  plt.legend(loc = "upper left")
  plt.axis([-4, 4, 0, 10])
  plt.show()
  y_pred = polynomial_regression.predict(X_test)
  score = r2_score(y_test, y_pred)
  print(score)

poly_regression(2)

