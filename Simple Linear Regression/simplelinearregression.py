# -*- coding: utf-8 -*-
"""SimpleLinearRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16tgu70GV5DDMnONSN90TWQNYH-UqjGlK
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
# %matplotlib inline

df = pd.read_csv("/content/Salary_dataset.csv")

df.head()

df.drop("Unnamed: 0", axis=1, inplace=True)

df.head()

df.info()

## Scatter Plot
plt.scatter(df["YearsExperience"], df["Salary"])
plt.xlabel("Years of Experience")
plt.ylabel("Salary")

## Correlation
df.corr()

## Seaborn for visualization
import seaborn as sns
sns.pairplot(df)

## Independent and Depenedent features
X = df[['YearsExperience']] ### Independent features should be data frame or 2 dimension not series
Y = df['Salary'] ### Can be a series
np.array(X).shape

X.head()

Y.head()

np.array(Y).shape

### Train Test Split
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=0)

X_train.head()

Y_train.head()

## Standardization
# y -> rupees As the units are different, in Gradient Descent values to come to global minima will take long time
# x -> years So we use Z Score, it will convert all values with mean =0 and SD =1
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train) # (fit_transform ) calculates with mean and std dev from train data
X_test = sc.transform(X_test)

X_train

# The same mean of Std Dev and Mean we use in test
# And the mean and Std Dev of train is used in test (transform)
X_test

## Apply Linear Regression
from sklearn.linear_model import LinearRegression
regression = LinearRegression()
regression.fit(X_train, Y_train)

## Here in the fit all the train must be in a 2d array

regression.coef_ #Slope

# y = B0 + B1 * x1
# B0 = Intercept
# B1 = Slope
regression.intercept_

"""# One unit movement in X axis it leads to 23476 movements in Y axis
# When X =0 Y is 72948
"""

## Plot training data plot best fit line
plt.scatter(X_train, Y_train, color="red")
plt.plot(X_train, regression.predict(X_train), color="blue")
plt.title("Salary vs Experience (Training Set)")
plt.xlabel("Years of Experience")
plt.ylabel("Salary")

## Prediction for test data
Y_pred = regression.predict(X_test)

"""y_pred_test = 72948.27 + 23476.54680309(X_test)


"""

## Performance Metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error
MAE = mean_absolute_error(Y_test, Y_pred)
MSE = mean_squared_error(Y_test, Y_pred)
RMSE = np.sqrt(MSE)

print(MAE)
print(MSE)
print(RMSE)

# R^2 = 1 - SSR / SST
from sklearn.metrics import r2_score
r2_score = r2_score(Y_test, Y_pred)

r2_score

# Plot test
plt.scatter(X_test, Y_test, color="red")
plt.plot(X_test, regression.predict(X_test), color="blue")
plt.title("Salary vs Experience (Training Set)")
plt.xlabel("Years of Experience")
plt.ylabel("Salary")

# Adjusted R2 = 1 - [(1 - R2)* (n-1)/ (n-k-1)]
# n = no of observations
# k = no of independent variables
1 - (1 - r2_score) * (len(Y_test) - 1) / (len(Y_test) - X_test.shape[1] - 1)

# OLS Linear Regression
import statsmodels.api as sm

model = sm.OLS(Y_train, X_train).fit()

prediction = model.predict(X_test)

prediction

print(model.summary())

# Step 1: Make the prediction (which is still in the scaled form)
scaled_prediction = regression.predict(sc.transform([[10]]))

# Step 2: Inverse transform the scaled prediction to get the original value
original_prediction = sc.inverse_transform(scaled_prediction.reshape(-1, 1))

print(original_prediction)

